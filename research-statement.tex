\PassOptionsToPackage{svgnames,dvipsnames,svgnames}{xcolor}
\documentclass[9pt]{extarticle}
\usepackage{microtype} 
% \usepackage{times}
\usepackage[T1]{fontenc}
\usepackage{soul}
\usepackage{mathpazo}
\usepackage[hmargin=1in,vmargin=1in]{geometry}
\usepackage{todonotes}
\usepackage{listings}
\lstset{tabsize=2, 
basicstyle=\ttfamily\fontsize{9pt}{1em}\selectfont, 
commentstyle=\itshape\ttfamily\color{gray}, 
stringstyle=\ttfamily\color{red},
mathescape=false,escapechar=\#,
numbers=left, numberstyle=\scriptsize\color{gray}\ttfamily, language=ML,moredelim=[il][\sffamily]{?},showspaces=false,showstringspaces=false,xleftmargin=15pt, morekeywords=[1]{tyfam,opfam,let,fn,val,def,casetype,objtype,metadata,of,*,datatype,new,toast,syntax,module,where,import,for,ana,syn,opcon,tycon,metasignature,metamodule,metasig,metamod,static,at,tycase,mod,macro,match,float,pattern,in,patterns,expressions,implicit,forall,rectype,fold,unfold,inj,by,spliced},deletekeywords={double},classoffset=0,belowskip=\smallskipamount,
moredelim=**[is][\color{red}]{SSTR}{ESTR},
moredelim=**[is][\color{Green}]{SHTML}{EHTML},
moredelim=**[is][\color{purple}]{SCSS}{ECSS},
moredelim=**[is][\color{brown}]{SSQL}{ESQL},
moredelim=**[is][\color{orange}]{SCOLOR}{ECOLOR},
moredelim=**[is][\color{magenta}]{SPCT}{EPCT}, 
moredelim=**[is][\color{gray}]{SNAT}{ENAT}, 
moredelim=**[is][\color{Green}]{SURL}{EURL},
moredelim=**[is][\color{SeaGreen}]{SQT}{EQT},
moredelim=**[is][\color{Periwinkle}]{SGRM}{EGRM},
moredelim=**[is][\color{YellowGreen}]{SID}{EID},
moredelim=**[is][\color{Sepia}]{SUS}{EUS},
deletestring=[d]{"},
}
\lstloadlanguages{Java,VBScript,XML,HTML,ML}
\let\li\lstinline

\usepackage{newtxtt}
\usepackage{enumitem}

\newcommand{\HazelEnv}{Hazel}

\begin{document}
\vspace{-8px}
\section*{Research Statement -- Cyrus Omar}
\vspace{-6px}
My research aims to bring to life a new generation of highly \emph{adaptable} and \emph{intelligent} programming systems.

An \emph{adaptable programming system} is one that allows the libraries that the programmer has imported to install new syntactic, semantic and edit-time features. Adaptability has become critical as programming has pervaded society, because language and tool designers can no longer hope to satisfyingly accomodate every problem domain. The challenge is in ensuring that these newly installed features do not interfere with one another, nor weaken the type and binding discipline of the language (because a language with a strong type and binding discipline is, I am thoroughly convinced, necessary for programming ``in the large''.) My research rigorously addresses these challenges.  % I have taken a leading role in the community of researchers aiming to address this problem. % At their core, the languages of the future will be based on the foundational principles of type theory.

An \emph{intelligent programming system} is one that combines a semantic understanding of the program being written with statistics gathered from other programs to synthesize code suggestions, and more generally \emph{action suggestions}, for the programmer throughout the programming process. There are many careers worth of problems that await confrontation here. My interest is in developing principled solutions to foundational problems, including (1) the problem of extracting meaning from programs that are not complete; (2) the problem of reasoning about edit actions as formal structures; and (3) the problem of combining semantic and statistical approaches to suggestion generation.

I am particularly interested in incorporating these mechanisms into languages and tools for (1) scientists, broadly construed; and (2) individuals with severely limited mobility and other disabilities. I have a distinctive perspective on these topics because as a student in neuroscience, (1) I developed computational models of neurobiological circuits in the sensorimotor system; and (2) I studied, and built, neural prosthetics.

% My approach is to start from the first principles of type theory, probability theory and information theory. I then apply disciplined qualitative and empirical methods as I scale up, validate and iterate on my designs.

\vspace{-8px}
\subsection*{Previous and Ongoing Research}
\vspace{-2px}
\paragraph{Adaptable Programming Systems}

Over the course of my graduate studies, I have developed several mechanisms that allow libraries to install new language and editor features, without running afoul of conflicts or semantic confusion.

My thesis research focuses on mechanisms that allow libraries to install type-specific syntactic sugar. For example, consider a web programming library that defines a type, \li{Element}, for HTML elements. Using the mechanisms I have developed, the library provider can equip this type with HTML syntax. Clients of this library can use this syntax wherever a value of type \li{Element} is expected, e.g.
\begin{lstlisting}[numbers=none]
val body : Element = `SURL<p>Hello, {[EURLjoin ' ' [first, last]SURL]}</p>EURL`
\end{lstlisting}
Uniquely, syntactic conflicts are impossible by construction, because the outer delimiters (here, backticks) are fixed, and control over parsing the body (between backticks) is delegated in a deterministic, type-directed manner. Moreover, the mechanism comes equipped with a principle of \emph{syntactic abstraction}, meaning that library clients can reason about types and binding without examining the expansions of newly installed syntactic forms like these. The core mechanism was formally introduced in a paper at \textbf{ECOOP 2014}, which was awarded a \textbf{Distinguished Paper Award}. It was  also covered widely by the popular programming press, and our implementation now has over 400 stars on GitHub. 
% A subsequent refinement of this core mechanism, in collaboration with an undergraduate summer student, was described in a short paper at \textbf{SAC 2015}. 
My dissertation formalizes variants of this mechanism that interact with advanced language features, including pattern matching, parameterized types and ML-style modules. An account of these subsequent advances was awarded \textbf{2nd place at the ICFP 2015 Student Research Competition}, and a capstone journal publication based on these results is in preparation for submission in the spring.

I have also developed a mechanism that gives library providers the ability to install new \emph{semantic fragments}. These fragment definitions are delegated control over typechecking and translation (to a fixed target language) in a type-directed manner. In a paper at \textbf{GPCE 2016}, we argue that this design is compositionally well-behaved and expressive, with examples of
fragments that express the static and dynamic semantics of
1) functional primitives (e.g. records and labeled sums, with support for nested pattern
matching \emph{a la} ML); 2) a variation on JavaScript's prototypal
object system; 3) typed foreign interfaces to Python and
OpenCL; and 4) a system of constrained string types useful for verifying that strings have been securely sanitized as intended. This last fragment, developed in collaboration with an undergraduate research student, was formalized in a paper at the \textbf{PSP 2014} workshop, where it was awarded the \textbf{Best Paper Award}. %Our implementation, \li{typy}, is embedded as a library into Python.

Finally, I have developed a mechanism that allows library providers to install type-specific {graphical user interfaces} for code generation into the client's program editor. Clients are offered access to these interfaces via the standard code completion menu based on the type of expression being entered at the cursor. In a paper published at \textbf{ICSE 2012}, we introduce this system and describe our human-centered design process. In particular, we began by gathering feedback on our initial mockups from over 450 professional software developers via an online survey. This resulted in a ranked collection of design criteria that guided our final design. We validated our final design with a controlled pilot study.

\vspace{-8px}
\paragraph{Intelligent Programming Systems}
Programming language definitions assign meaning to \emph{complete} programs. 
Programmers, however, spend a substantial amount of time interacting with \emph{incomplete} programs using tools like program editors and live programming environments (which interleave editing and evaluation.) These systems sometimes attempt to provide suggestions to the programmer. However, these suggestion systems are build largely around various \emph{ad hoc} heuristics. I am interested in designing programming systems that more intelligently suggest edit actions to the programmer. To build such a system, we need (1) to be able to extract meaning from incomplete programs; (2) to be reason about and compose structured edit actions; and (3) to develop a statistical model of incomplete programs and edit actions, so that the suggestions can be ranked by likelihood.

I have led the development of a foundational calculus called Hazelnut that addresses the first two of these problems. This calculus, described in a paper appearing at \textbf{POPL 2017}, assigns static meaning to programs that contain \emph{holes} and \emph{type inconsistencies}. Interestingly and encouragingly, the machinery for type holes coincides with that developed in the study of gradual type systems. The paper then develops an \emph{action semantics} that assigns meaning to structured edit actions and maintains a powerful invariant: that every well-defined action leaves the program statically meaningful, according to the aforementioned static semantics. We have mechanized the metatheory of Hazelnut using the Agda proof assistant. We also have a browser-based implementation of a simple {structure editor} based on this calculus. It is written in OCaml using functional reactive programming techniques, and compiled using \li{js_of_ocaml}.

With respect to the problem of developing statistical models of programs, I have led two projects of note. 
First, during my years as a neuroscientist, I developed a principled information theoretic framework for designing \emph{neural prosthetics}, i.e. systems used by paralyzed and locked-in individuals to communicate commands (i.e. simple programs) to various devices -- we used speech synthesizers, motorized wheelchairs, and (because everyone deserves to have some fun) prop airplanes. This framework, which was described in \textbf{IJHCI 2011}, relies on a statistical understanding of both the communication channel (in our system, an EEG headset) and  the language of commands.

The second project, which began as a course project for a Machine Learning course and remains ongoing as of this writing, involves developing a principled statistical model of well-typed expressions that takes into account the form, the type and the context around the expression being evaluated for insertion. In our preliminary investigations, our simple model already keeps pace with models that use an \emph{ad hoc} tokenized representation of programs \cite{icse-naturalness12}.

\vspace{-8px}
\paragraph{Computational Neuroscience and Neuroinformatics}

I entered graduate school at CMU in the Neural Computation PhD program. There, I worked in collaboration with an experimental neurophysiologist on a computational model of a neurobiological circuit in the portion of the rat somatosensory cortex responsible for whisker sensations. This research was published in the \textbf{Journal of Neuroscience (2012)}. I then became interested in the problem of systematically validating scientific models like these against empirical data on an ongoing basis. Our solution, which combines statistical testing techniques and modularity techniques, was described in a short paper at \textbf{ICSE 2014} (NIER track, 18\% acceptance rate.) My collaborator, Rick Gerkin, continues to push forward on this project, funded by an R01 grant from the National Institutes of Health. I contributed to the writing of this grant and continue to have an advisory role in the project. My transfer to the Computer Science program was precipitated by my experiences as a neuroscientist --- I became convinced that we need better languages and tools!


\vspace{-8px}
\subsection*{Future Research Directions}


Going forward, my research group, together with my present and future collaborators, will (1) continue to develop new adaptation and intelligent action suggestion mechanisms, starting from type theoretic and statistical first principles; and (2) integrate these advances into a coherent and principled next-generation \emph{live lab notebook} programming environment called Hazel. A mockup of the Hazel programming environment, which is organized much like the widely adopted Jupyter (formerly IPython) lab notebook, is shown in Figure \ref{fig:hazel-mockup}. Hazel will advance beyond Jupyter in several ways. 

First, Hazel is a \emph{structure editor}, meaning that programs are never left syntactically malformed. Instead, portions of the program that are not yet complete are explicitly marked with holes, indicated by rounded squares in the cell marked (a) in Figure \ref{fig:hazel-mockup}. In fact, Hazel, like Hazelnut, will go beyond syntactic well-formedness to maintain a stronger invariant: that every incomplete program that arises is also statically meaningful. This will require \textbf{scaling up the static semantics for incomplete programs} developed for Hazelnut (POPL 2017). I plan to do so by equipping Hazel with a modularly programmable static semantics, building upon my recent research in this area (GPCE 2016). In particular, I plan to develop a variant of this mechanism within Coq, and use Coq's extraction mechanism to generate an OCaml implementation, which will be compiled by \li{js_of_ocaml} (as in our present Hazelnut implementation.) This will allow us to make another fundamental advance: fragment providers will be able to develop \textbf{modular correctness proofs}.

In the cell marked (b) in Figure \ref{fig:hazel-mockup}, the programmer is constructing a value of matrix type. Hazel allows the programmer to construct a matrix value using a {two-dimensional projection}, as shown. The logic for displaying and interacting with this projection is not built in to Hazel, but rather installed by the \li{Numerics} library and triggered based on the type annotation. This mechanism of \textbf{type-specific projections} will build upon the advances made in my previous work on type-specific syntax (ECOOP 2014) and type-specific code generation interfaces (ICSE 2012).% Existing projectional editors offer only a fixed set of projections.

In the cell marked (c), the programmer applies \li{summary_stats} to the aforementioned matrix. In a standard programming environment, the fact that \li{summary_stats} is incomplete would disable evaluation. For Hazel, however, I plan is to investigate the problem of \textbf{evaluating incomplete programs}. Notice in cell (c) that the computation of the mean is able to proceed, and the computation of the standard deviation can proceed at least as far as is shown. This tightens the feedback loop for the client programmer, going beyond even what live programming environments today are capable of. It is challenging to get the dynamic semantics of incomplete programs right, because the standard notion of type safety breaks down -- evaluation can in fact ``get stuck''. I plan to develop a refined notion of type safety that precisely characterizes states that are appropriately stuck. I also plan to investigate the foundations of \textbf{edit and resume} functionality, which would allow the programmer to fill in holes and continue evaluation where it left off. This would be particularly useful for scientific computations involving large data sets. This line of research will build both upon the static semantics for incomplete programs we have developed (POPL 2017) and on work on contextual modal type theory (CMTT), which, by its correspondence with contextual modal logic, lays logical foundations beneath the problem of manipulating and reasoning about programs with holes \cite{Nanevski2008}.

The sidebar marked (d) offers feedback and action suggestions to the programmer. There are several fundamental research problems lurking behind this sidebar. The first was already discussed -- we need a semantics for edit actions. Our paper at POPL 2017 lays the foundations for Hazel's primitive action semantics. We next need a system that allows library providers to modularly install new \textbf{high-level edit actions}, defined in terms of the primitive actions. For example, it should be possible to implement various type-directed program generation and bug repair techniques as libraries, and integrate them directly into the editor, using this system. Each of these are substantial research areas in their own right -- I am already collaborating with Claire Le Goues at CMU, an expert on automated bug repair, and I plan to develop further collaborations in these directions in the future. Third, we need an \textbf{action suggestion semantics} that determines which suggestions are actually generated for any given edit state. This semantics must come equipped with a theorem that establishes that the suggestions offered are meaningful at the cursor (in blue in Figure \ref{fig:hazel-mockup}).  Finally, we need a \textbf{statistical model of actions} capable of ranking the suggested actions, as shown in the sidebar in Figure \ref{fig:hazel-mockup}. There are several theses worth of open problems on this front. I plan to start with a simple foundational statistical model (ongoing work, discussed above), and move forward to incorporate more sophisticated machine learning techniques, ideally in collaboration with researchers in machine learning. (This will be the basis for at least one large grant submission.)

\begin{figure}
\vspace{-6.5ex}
\centering
\includegraphics[width=0.8\textwidth]{mockup-1}
\vspace{-2.5ex}
\caption{A mockup of Hazel.}
\label{fig:hazel-mockup}
\vspace{-2ex}
\end{figure}

With Hazel, we are intentionally blurring the line between the programming language and the program editor. This opens up a number of interesting research directions in \textbf{language-editor co-design}. For example, it may be possible to recast  ``tricky'' language mechanisms, like function overloading, type classes, implicit values and unqualified imports, as editor mechanisms. Because we will be treating programming as a structured conversation between the programmer and the programming environment, the editor can simply ask the programmer to resolve ambiguities when they arise. The programmer's choice is then stored unambiguously in the syntax tree. %, so this mechanism is stable with respect to library evolution (unlike the analagous language mechanisms.) 

Another interesting problem that I plan to explore has to do with \textbf{semantic, interactive documentation}. In particular, in Hazel, references to program structures that appear in documentation will be treated in the same way as other references and be subject to renaming and other operations. Documentation will also be capable of containing expressions of arbitrary types (e.g. of the \li{Image} or \li{Link} type.) Together with the type-specific projection mechanism mentioned above, I hope that this will allow Hazel to function not only as a structured programming environment, but also as a structured authoring environment! Finally, I plan to explore the question of adding a spatiotemporal dimension to the documentation system, to help programmers construct structured tutorials of a codebase.

My research philosophy is to start with the first principles of type theory and probability theory. As such, my contributions will first take the form of core calculi, mechanized proofs and simple prototypes. That said, I am a big believer in applying disciplined qualitative and empirical methods to scale up, validate and iterate on my designs. In particular, I intend to make Hazel a practical tool for data science tasks, and to conduct pilot studies on its efficacy in this domain. These pilot studies will generat data that we plan to both publish and utilize internally as we iterate on Hazel's design. The ICSE 2012 paper described above demonstrates my experience with this approach. Taken together, I believe that my research has the potential to open up exciting new research directions for semanticists, tool designers and statisticians, both individually and in interdisciplinary collaboration. % It also has the potential to substantially improve the programming experience, for scientists and across a variety of other disciplines.

\bibliographystyle{abbrv}
 \bibliography{citations}
\end{document}
